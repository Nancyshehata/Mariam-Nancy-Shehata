{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378f5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as t\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "import matplotlib.pyplot as pt\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c9e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "Input = pd.read_csv('C:/Data/Time Series/TimeSeries.csv')\n",
    "Target = pd.read_csv('C:/Data/Image200x200/All cards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f56b5362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 40000)\n",
      "(28, 40000)\n",
      "(7, 40000)\n",
      "(28, 40000)\n",
      "(7, 40000)\n"
     ]
    }
   ],
   "source": [
    "Input_T = Input.T\n",
    "Target_T = Target.T\n",
    "X = Input_T.astype('float32')\n",
    "y = Target_T.astype('float32')\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "#X_train = torch.FloatTensor(X_train)\n",
    "#X_test = torch.FloatTensor(X_test)\n",
    "#y_train = torch.FloatTensor(y_train)\n",
    "#y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "print(X_train.shape); print(X_test.shape)\n",
    "print(y_train.shape); print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf48332c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000019BF7C06430>\n",
      "torch.Size([28, 40000])\n",
      "torch.Size([28, 40000])\n",
      "torch.Size([7, 40000])\n",
      "torch.Size([7, 40000])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "ArrayTrain_x = np.array(X_train) \n",
    "ArrayTrain_y = np.array(y_train) \n",
    "ArrayTest_x = np.array(X_test) \n",
    "ArrayTest_y  = np.array(y_test)\n",
    "TensorX_train = torch.FloatTensor(ArrayTrain_x)\n",
    "Tensory_train = torch.FloatTensor(ArrayTrain_y)\n",
    "Tensorx_test = torch.FloatTensor(ArrayTest_x)\n",
    "Tensory_test = torch.FloatTensor(ArrayTest_y)\n",
    "\n",
    "custom_train = (TensorX_train,Tensory_train) \n",
    "custom_test = (Tensorx_test,Tensory_test)\n",
    "\n",
    "\n",
    "trainloader = DataLoader(custom_train, batch_size=7, shuffle=True)\n",
    "testloader = DataLoader(custom_test, batch_size=3, shuffle=False)\n",
    "print (trainloader)\n",
    "print(TensorX_train.shape)\n",
    "print(Tensory_train.shape)\n",
    "print(Tensorx_test.shape)\n",
    "print(Tensory_test.shape)\n",
    "print(len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b04997ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MoonsModel(nn.Module):\n",
    "    def __init__(self, n_features, n_neurons):\n",
    "        super(MoonsModel, self).__init__()\n",
    "        self.hidden = nn.Linear(in_features=n_features, out_features=n_neurons)\n",
    "        self.out_layer = nn.Linear(in_features=n_neurons, out_features=40000)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = F.relu(self.hidden(X))\n",
    "        out = F.sigmoid(self.out_layer(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba0d2304",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nancy\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Step [1/1], Loss: 0.2533\n",
      "Epoch [2/500], Step [1/1], Loss: 0.2349\n",
      "Epoch [3/500], Step [1/1], Loss: 0.4091\n",
      "Epoch [4/500], Step [1/1], Loss: 0.3713\n",
      "Epoch [5/500], Step [1/1], Loss: 0.3318\n",
      "Epoch [6/500], Step [1/1], Loss: 0.2319\n",
      "Epoch [7/500], Step [1/1], Loss: 0.2744\n",
      "Epoch [8/500], Step [1/1], Loss: 0.2397\n",
      "Epoch [9/500], Step [1/1], Loss: 0.2186\n",
      "Epoch [10/500], Step [1/1], Loss: 0.1997\n",
      "Epoch [11/500], Step [1/1], Loss: 0.2244\n",
      "Epoch [12/500], Step [1/1], Loss: 0.1726\n",
      "Epoch [13/500], Step [1/1], Loss: 0.2188\n",
      "Epoch [14/500], Step [1/1], Loss: 0.1520\n",
      "Epoch [15/500], Step [1/1], Loss: 0.2106\n",
      "Epoch [16/500], Step [1/1], Loss: 0.2052\n",
      "Epoch [17/500], Step [1/1], Loss: 0.1353\n",
      "Epoch [18/500], Step [1/1], Loss: 0.1288\n",
      "Epoch [19/500], Step [1/1], Loss: 0.1858\n",
      "Epoch [20/500], Step [1/1], Loss: 0.1174\n",
      "Epoch [21/500], Step [1/1], Loss: 0.1707\n",
      "Epoch [22/500], Step [1/1], Loss: 0.1619\n",
      "Epoch [23/500], Step [1/1], Loss: 0.1065\n",
      "Epoch [24/500], Step [1/1], Loss: 0.1005\n",
      "Epoch [25/500], Step [1/1], Loss: 0.1342\n",
      "Epoch [26/500], Step [1/1], Loss: 0.0924\n",
      "Epoch [27/500], Step [1/1], Loss: 0.1157\n",
      "Epoch [28/500], Step [1/1], Loss: 0.1060\n",
      "Epoch [29/500], Step [1/1], Loss: 0.0872\n",
      "Epoch [30/500], Step [1/1], Loss: 0.0866\n",
      "Epoch [31/500], Step [1/1], Loss: 0.0781\n",
      "Epoch [32/500], Step [1/1], Loss: 0.0692\n",
      "Epoch [33/500], Step [1/1], Loss: 0.0610\n",
      "Epoch [34/500], Step [1/1], Loss: 0.0531\n",
      "Epoch [35/500], Step [1/1], Loss: 0.0457\n",
      "Epoch [36/500], Step [1/1], Loss: 0.0389\n",
      "Epoch [37/500], Step [1/1], Loss: 0.0722\n",
      "Epoch [38/500], Step [1/1], Loss: 0.0642\n",
      "Epoch [39/500], Step [1/1], Loss: 0.0241\n",
      "Epoch [40/500], Step [1/1], Loss: 0.0207\n",
      "Epoch [41/500], Step [1/1], Loss: 0.0178\n",
      "Epoch [42/500], Step [1/1], Loss: 0.0565\n",
      "Epoch [43/500], Step [1/1], Loss: 0.0135\n",
      "Epoch [44/500], Step [1/1], Loss: 0.0119\n",
      "Epoch [45/500], Step [1/1], Loss: 0.0527\n",
      "Epoch [46/500], Step [1/1], Loss: 0.0514\n",
      "Epoch [47/500], Step [1/1], Loss: 0.0088\n",
      "Epoch [48/500], Step [1/1], Loss: 0.0082\n",
      "Epoch [49/500], Step [1/1], Loss: 0.0486\n",
      "Epoch [50/500], Step [1/1], Loss: 0.0478\n",
      "Epoch [51/500], Step [1/1], Loss: 0.0069\n",
      "Epoch [52/500], Step [1/1], Loss: 0.0066\n",
      "Epoch [53/500], Step [1/1], Loss: 0.0063\n",
      "Epoch [54/500], Step [1/1], Loss: 0.0061\n",
      "Epoch [55/500], Step [1/1], Loss: 0.0460\n",
      "Epoch [56/500], Step [1/1], Loss: 0.0058\n",
      "Epoch [57/500], Step [1/1], Loss: 0.0455\n",
      "Epoch [58/500], Step [1/1], Loss: 0.0452\n",
      "Epoch [59/500], Step [1/1], Loss: 0.0448\n",
      "Epoch [60/500], Step [1/1], Loss: 0.0054\n",
      "Epoch [61/500], Step [1/1], Loss: 0.0053\n",
      "Epoch [62/500], Step [1/1], Loss: 0.0052\n",
      "Epoch [63/500], Step [1/1], Loss: 0.0052\n",
      "Epoch [64/500], Step [1/1], Loss: 0.0440\n",
      "Epoch [65/500], Step [1/1], Loss: 0.0440\n",
      "Epoch [66/500], Step [1/1], Loss: 0.0438\n",
      "Epoch [67/500], Step [1/1], Loss: 0.0436\n",
      "Epoch [68/500], Step [1/1], Loss: 0.0434\n",
      "Epoch [69/500], Step [1/1], Loss: 0.0432\n",
      "Epoch [70/500], Step [1/1], Loss: 0.0050\n",
      "Epoch [71/500], Step [1/1], Loss: 0.0049\n",
      "Epoch [72/500], Step [1/1], Loss: 0.0429\n",
      "Epoch [73/500], Step [1/1], Loss: 0.0428\n",
      "Epoch [74/500], Step [1/1], Loss: 0.0427\n",
      "Epoch [75/500], Step [1/1], Loss: 0.0427\n",
      "Epoch [76/500], Step [1/1], Loss: 0.0048\n",
      "Epoch [77/500], Step [1/1], Loss: 0.0048\n",
      "Epoch [78/500], Step [1/1], Loss: 0.0425\n",
      "Epoch [79/500], Step [1/1], Loss: 0.0048\n",
      "Epoch [80/500], Step [1/1], Loss: 0.0423\n",
      "Epoch [81/500], Step [1/1], Loss: 0.0048\n",
      "Epoch [82/500], Step [1/1], Loss: 0.0422\n",
      "Epoch [83/500], Step [1/1], Loss: 0.0422\n",
      "Epoch [84/500], Step [1/1], Loss: 0.0047\n",
      "Epoch [85/500], Step [1/1], Loss: 0.0421\n",
      "Epoch [86/500], Step [1/1], Loss: 0.0047\n",
      "Epoch [87/500], Step [1/1], Loss: 0.0047\n",
      "Epoch [88/500], Step [1/1], Loss: 0.0420\n",
      "Epoch [89/500], Step [1/1], Loss: 0.0047\n",
      "Epoch [90/500], Step [1/1], Loss: 0.0046\n",
      "Epoch [91/500], Step [1/1], Loss: 0.0046\n",
      "Epoch [92/500], Step [1/1], Loss: 0.0417\n",
      "Epoch [93/500], Step [1/1], Loss: 0.0417\n",
      "Epoch [94/500], Step [1/1], Loss: 0.0416\n",
      "Epoch [95/500], Step [1/1], Loss: 0.0415\n",
      "Epoch [96/500], Step [1/1], Loss: 0.0046\n",
      "Epoch [97/500], Step [1/1], Loss: 0.0046\n",
      "Epoch [98/500], Step [1/1], Loss: 0.0412\n",
      "Epoch [99/500], Step [1/1], Loss: 0.0412\n",
      "Epoch [100/500], Step [1/1], Loss: 0.0046\n",
      "Epoch [101/500], Step [1/1], Loss: 0.0411\n",
      "Epoch [102/500], Step [1/1], Loss: 0.0411\n",
      "Epoch [103/500], Step [1/1], Loss: 0.0045\n",
      "Epoch [104/500], Step [1/1], Loss: 0.0045\n",
      "Epoch [105/500], Step [1/1], Loss: 0.0410\n",
      "Epoch [106/500], Step [1/1], Loss: 0.0410\n",
      "Epoch [107/500], Step [1/1], Loss: 0.0045\n",
      "Epoch [108/500], Step [1/1], Loss: 0.0045\n",
      "Epoch [109/500], Step [1/1], Loss: 0.0045\n",
      "Epoch [110/500], Step [1/1], Loss: 0.0045\n",
      "Epoch [111/500], Step [1/1], Loss: 0.0045\n",
      "Epoch [112/500], Step [1/1], Loss: 0.0045\n",
      "Epoch [113/500], Step [1/1], Loss: 0.0409\n",
      "Epoch [114/500], Step [1/1], Loss: 0.0408\n",
      "Epoch [115/500], Step [1/1], Loss: 0.0408\n",
      "Epoch [116/500], Step [1/1], Loss: 0.0407\n",
      "Epoch [117/500], Step [1/1], Loss: 0.0045\n",
      "Epoch [118/500], Step [1/1], Loss: 0.0045\n",
      "Epoch [119/500], Step [1/1], Loss: 0.0045\n",
      "Epoch [120/500], Step [1/1], Loss: 0.0407\n",
      "Epoch [121/500], Step [1/1], Loss: 0.0407\n",
      "Epoch [122/500], Step [1/1], Loss: 0.0406\n",
      "Epoch [123/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [124/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [125/500], Step [1/1], Loss: 0.0406\n",
      "Epoch [126/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [127/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [128/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [129/500], Step [1/1], Loss: 0.0406\n",
      "Epoch [130/500], Step [1/1], Loss: 0.0406\n",
      "Epoch [131/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [132/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [133/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [134/500], Step [1/1], Loss: 0.0405\n",
      "Epoch [135/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [136/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [137/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [138/500], Step [1/1], Loss: 0.0404\n",
      "Epoch [139/500], Step [1/1], Loss: 0.0404\n",
      "Epoch [140/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [141/500], Step [1/1], Loss: 0.0404\n",
      "Epoch [142/500], Step [1/1], Loss: 0.0404\n",
      "Epoch [143/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [144/500], Step [1/1], Loss: 0.0404\n",
      "Epoch [145/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [146/500], Step [1/1], Loss: 0.0404\n",
      "Epoch [147/500], Step [1/1], Loss: 0.0403\n",
      "Epoch [148/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [149/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [150/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [151/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [152/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [153/500], Step [1/1], Loss: 0.0402\n",
      "Epoch [154/500], Step [1/1], Loss: 0.0044\n",
      "Epoch [155/500], Step [1/1], Loss: 0.0402\n",
      "Epoch [156/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [157/500], Step [1/1], Loss: 0.0401\n",
      "Epoch [158/500], Step [1/1], Loss: 0.0401\n",
      "Epoch [159/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [160/500], Step [1/1], Loss: 0.0401\n",
      "Epoch [161/500], Step [1/1], Loss: 0.0400\n",
      "Epoch [162/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [163/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [164/500], Step [1/1], Loss: 0.0400\n",
      "Epoch [165/500], Step [1/1], Loss: 0.0400\n",
      "Epoch [166/500], Step [1/1], Loss: 0.0400\n",
      "Epoch [167/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [168/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [169/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [170/500], Step [1/1], Loss: 0.0400\n",
      "Epoch [171/500], Step [1/1], Loss: 0.0400\n",
      "Epoch [172/500], Step [1/1], Loss: 0.0399\n",
      "Epoch [173/500], Step [1/1], Loss: 0.0399\n",
      "Epoch [174/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [175/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [176/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [177/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [178/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [179/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [180/500], Step [1/1], Loss: 0.0399\n",
      "Epoch [181/500], Step [1/1], Loss: 0.0399\n",
      "Epoch [182/500], Step [1/1], Loss: 0.0399\n",
      "Epoch [183/500], Step [1/1], Loss: 0.0399\n",
      "Epoch [184/500], Step [1/1], Loss: 0.0399\n",
      "Epoch [185/500], Step [1/1], Loss: 0.0398\n",
      "Epoch [186/500], Step [1/1], Loss: 0.0398\n",
      "Epoch [187/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [188/500], Step [1/1], Loss: 0.0398\n",
      "Epoch [189/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [190/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [191/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [192/500], Step [1/1], Loss: 0.0398\n",
      "Epoch [193/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [194/500], Step [1/1], Loss: 0.0397\n",
      "Epoch [195/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [196/500], Step [1/1], Loss: 0.0397\n",
      "Epoch [197/500], Step [1/1], Loss: 0.0397\n",
      "Epoch [198/500], Step [1/1], Loss: 0.0397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [199/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [200/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [201/500], Step [1/1], Loss: 0.0397\n",
      "Epoch [202/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [203/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [204/500], Step [1/1], Loss: 0.0396\n",
      "Epoch [205/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [206/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [207/500], Step [1/1], Loss: 0.0396\n",
      "Epoch [208/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [209/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [210/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [211/500], Step [1/1], Loss: 0.0396\n",
      "Epoch [212/500], Step [1/1], Loss: 0.0396\n",
      "Epoch [213/500], Step [1/1], Loss: 0.0396\n",
      "Epoch [214/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [215/500], Step [1/1], Loss: 0.0396\n",
      "Epoch [216/500], Step [1/1], Loss: 0.0396\n",
      "Epoch [217/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [218/500], Step [1/1], Loss: 0.0396\n",
      "Epoch [219/500], Step [1/1], Loss: 0.0396\n",
      "Epoch [220/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [221/500], Step [1/1], Loss: 0.0396\n",
      "Epoch [222/500], Step [1/1], Loss: 0.0396\n",
      "Epoch [223/500], Step [1/1], Loss: 0.0396\n",
      "Epoch [224/500], Step [1/1], Loss: 0.0396\n",
      "Epoch [225/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [226/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [227/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [228/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [229/500], Step [1/1], Loss: 0.0395\n",
      "Epoch [230/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [231/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [232/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [233/500], Step [1/1], Loss: 0.0395\n",
      "Epoch [234/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [235/500], Step [1/1], Loss: 0.0395\n",
      "Epoch [236/500], Step [1/1], Loss: 0.0395\n",
      "Epoch [237/500], Step [1/1], Loss: 0.0395\n",
      "Epoch [238/500], Step [1/1], Loss: 0.0395\n",
      "Epoch [239/500], Step [1/1], Loss: 0.0395\n",
      "Epoch [240/500], Step [1/1], Loss: 0.0043\n",
      "Epoch [241/500], Step [1/1], Loss: 0.0395\n",
      "Epoch [242/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [243/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [244/500], Step [1/1], Loss: 0.0394\n",
      "Epoch [245/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [246/500], Step [1/1], Loss: 0.0394\n",
      "Epoch [247/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [248/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [249/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [250/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [251/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [252/500], Step [1/1], Loss: 0.0394\n",
      "Epoch [253/500], Step [1/1], Loss: 0.0394\n",
      "Epoch [254/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [255/500], Step [1/1], Loss: 0.0394\n",
      "Epoch [256/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [257/500], Step [1/1], Loss: 0.0394\n",
      "Epoch [258/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [259/500], Step [1/1], Loss: 0.0394\n",
      "Epoch [260/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [261/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [262/500], Step [1/1], Loss: 0.0394\n",
      "Epoch [263/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [264/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [265/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [266/500], Step [1/1], Loss: 0.0394\n",
      "Epoch [267/500], Step [1/1], Loss: 0.0394\n",
      "Epoch [268/500], Step [1/1], Loss: 0.0394\n",
      "Epoch [269/500], Step [1/1], Loss: 0.0394\n",
      "Epoch [270/500], Step [1/1], Loss: 0.0393\n",
      "Epoch [271/500], Step [1/1], Loss: 0.0393\n",
      "Epoch [272/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [273/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [274/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [275/500], Step [1/1], Loss: 0.0393\n",
      "Epoch [276/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [277/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [278/500], Step [1/1], Loss: 0.0393\n",
      "Epoch [279/500], Step [1/1], Loss: 0.0393\n",
      "Epoch [280/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [281/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [282/500], Step [1/1], Loss: 0.0393\n",
      "Epoch [283/500], Step [1/1], Loss: 0.0393\n",
      "Epoch [284/500], Step [1/1], Loss: 0.0393\n",
      "Epoch [285/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [286/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [287/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [288/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [289/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [290/500], Step [1/1], Loss: 0.0393\n",
      "Epoch [291/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [292/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [293/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [294/500], Step [1/1], Loss: 0.0392\n",
      "Epoch [295/500], Step [1/1], Loss: 0.0392\n",
      "Epoch [296/500], Step [1/1], Loss: 0.0392\n",
      "Epoch [297/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [298/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [299/500], Step [1/1], Loss: 0.0392\n",
      "Epoch [300/500], Step [1/1], Loss: 0.0392\n",
      "Epoch [301/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [302/500], Step [1/1], Loss: 0.0392\n",
      "Epoch [303/500], Step [1/1], Loss: 0.0392\n",
      "Epoch [304/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [305/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [306/500], Step [1/1], Loss: 0.0392\n",
      "Epoch [307/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [308/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [309/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [310/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [311/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [312/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [313/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [314/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [315/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [316/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [317/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [318/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [319/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [320/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [321/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [322/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [323/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [324/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [325/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [326/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [327/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [328/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [329/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [330/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [331/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [332/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [333/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [334/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [335/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [336/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [337/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [338/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [339/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [340/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [341/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [342/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [343/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [344/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [345/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [346/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [347/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [348/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [349/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [350/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [351/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [352/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [353/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [354/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [355/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [356/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [357/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [358/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [359/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [360/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [361/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [362/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [363/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [364/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [365/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [366/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [367/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [368/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [369/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [370/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [371/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [372/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [373/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [374/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [375/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [376/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [377/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [378/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [379/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [380/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [381/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [382/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [383/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [384/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [385/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [386/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [387/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [388/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [389/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [390/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [391/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [392/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [393/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [394/500], Step [1/1], Loss: 0.0391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [395/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [396/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [397/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [398/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [399/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [400/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [401/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [402/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [403/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [404/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [405/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [406/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [407/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [408/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [409/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [410/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [411/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [412/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [413/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [414/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [415/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [416/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [417/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [418/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [419/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [420/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [421/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [422/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [423/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [424/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [425/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [426/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [427/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [428/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [429/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [430/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [431/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [432/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [433/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [434/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [435/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [436/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [437/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [438/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [439/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [440/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [441/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [442/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [443/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [444/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [445/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [446/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [447/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [448/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [449/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [450/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [451/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [452/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [453/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [454/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [455/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [456/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [457/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [458/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [459/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [460/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [461/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [462/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [463/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [464/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [465/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [466/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [467/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [468/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [469/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [470/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [471/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [472/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [473/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [474/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [475/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [476/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [477/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [478/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [479/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [480/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [481/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [482/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [483/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [484/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [485/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [486/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [487/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [488/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [489/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [490/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [491/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [492/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [493/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [494/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [495/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [496/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [497/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [498/500], Step [1/1], Loss: 0.0042\n",
      "Epoch [499/500], Step [1/1], Loss: 0.0391\n",
      "Epoch [500/500], Step [1/1], Loss: 0.0042\n"
     ]
    }
   ],
   "source": [
    "model = MoonsModel(n_features=40000, n_neurons=300)\n",
    "criterion = nn.MSELoss()\n",
    "loss = 0.0\n",
    "\n",
    "from torch import optim\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)   \n",
    "optimizer\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "num_epochs = 500\n",
    "losses = []\n",
    "\n",
    "def train(num_epochs, model, trainloader):\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(trainloader)\n",
    "    print(total_step)\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (TensorX_train, Tensory_train) in enumerate(trainloader):\n",
    "            \n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "\n",
    "            outputs = model(TensorX_train)               \n",
    "            loss = criterion(outputs, Tensory_train)\n",
    "\n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()           \n",
    "            \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()    \n",
    "            losses.append(loss.item())\n",
    "            # apply gradients             \n",
    "            optimizer.step()                \n",
    "            \n",
    "            if (i+1) % 1 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "               \n",
    "\n",
    "train(num_epochs, model, trainloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d246174e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA1UklEQVR4nO3deZwc9Xng/8/T51wajY5BEpJAB+I0iEMGDBiMD8yRWLDGBh8ksfEqOODESbwxJNnsL/FvHdtJvHEcYpklxGZtBzsBJawtDDZ2bAiHNRKnAIGQBBpJSKNrRpqZvp/9o6q6q6ure1piSo1mnvfrpZe66+pv1XTXU8/3+61viapijDHGBMVaXQBjjDFvTRYgjDHGhLIAYYwxJpQFCGOMMaEsQBhjjAmVaHUBxtPMmTN1wYIFrS6GMcYcNdauXbtbVXvD5k2oALFgwQL6+vpaXQxjjDlqiMhr9eZZFZMxxphQFiCMMcaEsgBhjDEmlAUIY4wxoSxAGGOMCWUBwhhjTCgLEMYYY0JZgHA91z/IM1v3t7oYxhjzljGhbpR7M3797x8FYMuXrmpxSYwx5q3BMghjjDGhIg0QInK5iGwQkY0icmuD5d4uIkURufZQ1zXGGBONyAKEiMSB24ErgFOBj4jIqXWW+zLw4KGua4wxJjpRZhDnAhtVdZOq5oB7gOUhy30GuBfYdRjrGmOMiUiUAWIusNX3vt+dViYic4FrgJWHuq5vGytEpE9E+gYGBt50oY0xxjiiDBASMk0D7/8W+LyqFg9jXWei6h2qukxVl/X2hg5pbowx5jBE2c21H5jvez8P2B5YZhlwj4gAzASuFJFCk+tGQlVxy2OMMZNalAFiDbBERBYC24DrgY/6F1DVhd5rEfkW8ENV/TcRSYy1blRyxRLpRPxIfJQxxrylRRYgVLUgIrfg9E6KA3ep6noRucmdH2x3GHPdqMrql8lbgDDGGIj4TmpVXQ2sDkwLDQyq+ltjrXskZAtFIHmkP9YYY95y7E7qgGy+1OoiGGPMW4IFiIBswQKEMcaABYgamXywx60xxkxOFiACLIMwxhiHBYgAp5HaGGOMBYgAa6Q2xhiHBYgAyyCMMcZhASLA2iCMMcZhASLAqpiMMcZhAcKVijuHImNVTMYYA1iAKEslnENhGYQxxjgsQLjKAcIyCGOMASxAlMXcR0BkLIMwxhjAAkSZus+rswzCGGMcFiBcJTdCWDdXY4xxRBogRORyEdkgIhtF5NaQ+ctF5FkReVpE+kTkIt+8LSLynDcvynIClNwMwgbrM8YYR2QPDBKROHA78D6cZ0yvEZH7VfUF32IPA/erqorIGcAPgJN98y9V1d1RldHPMghjjKkWZQZxLrBRVTepag64B1juX0BVD6p6tf90AkqreG0Q1khtjDFAtAFiLrDV977fnVZFRK4RkZeAHwGf9M1S4CERWSsiK+p9iIiscKun+gYGBg67sJUMwqqYjDEGog0QEjKtJkNQ1VWqejJwNfAF36wLVfVs4ArgZhG5OOxDVPUOVV2mqst6e3sPu7CVNgjLIIwxBqINEP3AfN/7ecD2egur6i+BxSIy032/3f1/F7AKp8oqMpZBGGNMtSgDxBpgiYgsFJEUcD1wv38BETlBRMR9fTaQAvaISKeITHGndwKXAc9HWFbffRCWQRhjDETYi0lVCyJyC/AgEAfuUtX1InKTO38l8EHgN0QkD4wC17k9mmYBq9zYkQC+p6o/jqqsUMkgrJurMcY4IgsQAKq6GlgdmLbS9/rLwJdD1tsELI2ybDWf6f5vGYQxxjjsTmqX3QdhjDHVLEAAqlppg7AqJmOMASxAAJUGaoCMZRDGGANYgAAq1UsA+aIFCGOMAQsQQOUmuWRcUHWqnIwxZrKzAAGo24cp7j41qFiyAGGMMRYgqLRBJGPO4ShaBmGMMRYgoNIGkYg7GUTJmiGMMcYCBPjbIJzDUbAIYYwxFiCgkkF4AcLigzHGWIAAQN2A4FUxWRuEMcZYgACsF5MxxoSxAIGvDcLtxVSyDMIYYyxAgK8NIuFkEAXLIIwxxgIE+Lq5ehmEBQhjjIk2QIjI5SKyQUQ2isitIfOXi8izIvK0iPSJyEXNrjue1DfUBlgbhDHGQIQBQkTiwO3AFcCpwEdE5NTAYg8DS1X1TOCTwJ2HsO64CWYQ1ovJGGOizSDOBTaq6iZVzQH3AMv9C6jqQa2MjNdJ5cFuY647nrwSVO6ktgBhjDFRBoi5wFbf+353WhURuUZEXgJ+hJNFNL2uu/4Kt3qqb2Bg4LAKGrxRzhqpjTEm2gAhIdNqzryqukpVTwauBr5wKOu669+hqstUdVlvb+9hFbScQdh9EMYYUxZlgOgH5vvezwO211tYVX8JLBaRmYe67ptVM9SGtUEYY0ykAWINsEREFopICrgeuN+/gIicICLivj4bSAF7mll3PJUCbRCWQRhjDCSi2rCqFkTkFuBBIA7cparrReQmd/5K4IPAb4hIHhgFrnMbrUPXjaqsNfdBWAZhjDHRBQgAVV0NrA5MW+l7/WXgy82uG5XgfRCFogUIY4yxO6mpPIPaRnM1xpgKCxD42iBi9jwIY4zxWIDA34vJMghjjPFYgMD/TGobrM8YYzwWIPA1UsdsuG9jjPFYgMA/FpM7WJ8FCGOMsQAB/iomJ4P47PefYiRXaGWRjDGm5SxA4GukdnsxZfIl7l23rZVFMsaYlrMAQe1QG0Cl3skYYyYpCxBUbpTzBuuDStAwxpjJygIElWCQ9GUQNh6TMWayswBBJRjEY5ZBGGOMxwIEtYP1OdMsQhhjJjcLEPgG6/NlEBYfjDGTnQUIwtsgckUbsc8YM7lZgKD2kaMAmXyxVcUxxpi3hEgDhIhcLiIbRGSjiNwaMv9jIvKs++8xEVnqm7dFRJ4TkadFpC+qMqoqd/xyE1B9H4QFCGPMZBfZE+VEJA7cDrwP6AfWiMj9qvqCb7HNwCWquk9ErgDuAM7zzb9UVXdHVUa3nDy60fkIfxtEJm9VTMaYyS3KDOJcYKOqblLVHHAPsNy/gKo+pqr73LdPAPMiLM+YLIMwxpiKKAPEXGCr732/O62eG4EHfO8VeEhE1orIinoricgKEekTkb6BgYE3VeCqNoiCZRDGmMktsiomQEKmhXYeFZFLcQLERb7JF6rqdhE5BviJiLykqr+s2aDqHThVUyxbtuxNdU5NxCpFHs1ZBmGMmdyizCD6gfm+9/OA7cGFROQM4E5guaru8aar6nb3/13AKpwqq0iIGxdiUgkQ2YIFCGPM5BZlgFgDLBGRhSKSAq4H7vcvICLHAfcBN6jqy77pnSIyxXsNXAY8H1VBu9uSAPjaqK0Nwhgz6UVWxaSqBRG5BXgQiAN3qep6EbnJnb8S+DNgBvAP4ly9F1R1GTALWOVOSwDfU9UfR1XW7vYEg6N5xFcrNmoBwhgzyUXZBoGqrgZWB6at9L3+FPCpkPU2AUuD06PiZBCj5HwN0yNZCxDGmMnN7qQGprQ5cXIoky9P8782xpjJyAIE8NsXLwZgyayu8rSh0YKN6GqMmdQirWI6Wlx68jFs+dJVVdNyxRLZQom2ZLxFpTLGmNayDCLgOzeex4qLFwEwNGrVTMaYycsyiICLlsxk70gOcNohjulua3GJjDGmNSyDCNHtNlo/s3WQgQPZFpfGGGNawwJEiO5258a5P/yXZ3jnV37W4tIYY0xrWIAI4d1ZDTbstzFm8rIAEaK7vbppZvVzO1pUEmOMaZ2mAoSI/J6IdIvjH0VknYhcFnXhWsWfQQD8znfXtagkxhjTOs1mEJ9U1SGcQfN6gU8AX4qsVC1m9z4YY0zzAcIbxe5K4J9U9RnCn/dgjDFmgmg2QKwVkYdwAsSD7lDck6r19vafb6RUsqE3jDGTR7MB4kbgVuDtqjoCJHGqmSaNv3pwA09u3tvqYhhjzBHTbIB4B7BBVfeLyMeBPwUGoytW68VCKtCe2rrvyBfEGGNapNkA8Q1gRESWAn8EvAbcPdZKInK5iGwQkY0icmvI/I+JyLPuv8fc7Te1btTiIRHi+W0TOiYaY0yVZgNEQZ2xr5cDX1PVrwFTGq0gInHgduAK4FTgIyJyamCxzcAlqnoG8AXgjkNYN1L+51N7hu0hQsaYSaTZAHFARG4DbgB+5J7Ak2Oscy6wUVU3qWoOuAcnwJSp6mOq6tXbPAHMa3bdqIVlECV7PoQxZhJpNkBcB2Rx7od4A5gL/NUY68wFtvre97vT6rkReOBQ1xWRFSLSJyJ9AwMDYxSpeR9YemzNtKL1YjLGTCJNBQg3KHwXmCoivwZkVHWsNoiw+yRCz7AicilOgPj8oa6rqneo6jJVXdbb2ztGkZr3havfxjVnVcekQtEChDFm8mh2qI0PA78CPgR8GHhSRK4dY7V+YL7v/Txge8i2zwDuBJar6p5DWTdKyXiMY3uqnwWRL02qWz+MMZNcsw8M+hOceyB2AYhIL/BT4F8brLMGWCIiC4FtwPXAR/0LiMhxwH3ADar68qGseyQk49Xx06qYjDGTSbMBIuYFB9cexsg+VLUgIrcADwJx4C5VXS8iN7nzVwJ/BswA/kGcXkMFt7oodN1D2bHxEAwQeatiMsZMIs0GiB+LyIPAP7vvrwNWj7WSqq4OLucGBu/1p4BPNbtuqxWtiskYM4k0FSBU9b+JyAeBC3EakO9Q1VWRluwtqFBUfv7SLk6Z083sqfasamPMxNZsBoGq3gvcG2FZ3nI0cN9DoaR84ltrmDO1jcdve0+LSmWMMUdGwwAhIgcI714qgKpqdySleosI3heXKzhVTDsGMy0ojTHGHFkNA4SqNhxOY7IZzdtQG8aYycOeSd1AMHWyAGGMmUwsQDRQr4rJGGMmAwsQDWj46B7GGDMpWIBoYFZ3/a6s+aJlE8aYic0CRAPXLZvPp9+1OHTeO/7yZ0e4NMYYc2RZgGggFhN+/YzaYb8Bdh/MHuHSGGPMkWUBYgyJeNjI445swXo1GWMmLgsQY0iEPFnOs3Xv6BEsiTHGHFkWIMYQHNHVb8vu4SNYEmOMObIsQIyhURXTUCZ/BEtijDFHlgWIMSRi9Q+RdXU1xkxkFiDG0KgNImcPEDLGTGCRBggRuVxENojIRhG5NWT+ySLyuIhkReRzgXlbROQ5EXlaRPqiLGcj8QZVTAXLIIwxE1jTz4M4VCISB24H3gf0A2tE5H5VfcG32F7gd4Gr62zmUlXdHVUZm9Eog7AqJmPMRBZlBnEusFFVN6lqDrgHWO5fQFV3qeoa4C3b2tuRSvBPv/V2vrD8tJp59oxqY8xEFmWAmAts9b3vd6c1S4GHRGStiKyot5CIrBCRPhHpGxgYOMyiNnbpycfQO6V2XCYb3dUYM5FFGSDC6mYO5ZL7QlU9G7gCuFlELg5bSFXvUNVlqrqst7f3cMrZlGRIW4RVMRljJrIoA0Q/MN/3fh6wvdmVVXW7+/8uYBVOlVXLxEPaIgolq2IyxkxcUQaINcASEVkoIingeuD+ZlYUkU4RmeK9Bi4Dno+spE0Iu6PaqpiMMRNZZL2YVLUgIrcADwJx4C5VXS8iN7nzV4rIbKAP6AZKIvJZ4FRgJrBKRLwyfk9VfxxVWZsRlkFYFZMxZiKLLEAAqOpqYHVg2krf6zdwqp6ChoClUZbtUFkbhDFmsrE7qZsUDxlyI19UXtg+1ILSGGNM9CxANGnO1Npurque2saVf/cIa1/b24ISGWNMtCxANOmYKem68/r32XMhjDETjwWIJrkN5oc8zxhjjlYWIA7BN284J3R6g+GajDHmqGUB4hC8/7TZXHxi7d3aMcsgjDETkAWIQ6Rae/d0KWSaMcYc7SxAHCIvFvhvnLM7qo0xE5EFiEOk7niDKd/QGxYgjDETkQWIQ3TirClA9X0RL71xgKIN3GeMmWAsQByi2644he+vOJ/T500tT/vWY1v4nz96sYWlMsaY8WcB4hClEjHOWzSjZnTXB57f0aISGWNMNCxAHKZggNg3kmtRSYwxJhoWIA5TKjC6ayZvDdXGmInFAsRhCnuAkDHGTCSRnuVE5HIR2SAiG0Xk1pD5J4vI4yKSFZHPHcq6rTY7ZHTXTL7YgpIYY0w0IgsQIhIHbgeuwHlK3EdE5NTAYnuB3wX++jDWbaml83tqpg2N5o98QYwxJiJRZhDnAhtVdZOq5oB7gOX+BVR1l6quAYJn1jHXbbXTju2umTZoAcIYM4FEGSDmAlt97/vdaeO6roisEJE+EekbGBg4rIIejo5Ugq988IyqaRYgjDETSZQBImyI02ZvN256XVW9Q1WXqeqy3t7akVaj9OG3z696bwHCGDORRBkg+gH/GXQesP0IrNsyFiCMMRNJlAFiDbBERBaKSAq4Hrj/CKzbMhYgjDETSSKqDatqQURuAR4E4sBdqrpeRG5y568UkdlAH9ANlETks8CpqjoUtm5UZX0z7v30BYzkCtzwj79i/4gFCGPMxBFZgABQ1dXA6sC0lb7Xb+BUHzW17lvROcdPA2BKOsHWvSMtLo0xxowfux14nCzs7eS+p7axaeBgq4tijDHjwgLEOPmzX3Pu43t66/7WFsQYY8aJBYhxcsa8HmICm3cPt7ooxhgzLixAjJNUIsb86R1ssgBhjJkgLECMo0UzO9k8YAHCGDMxWIAYRwtndrF59zAlez61MWYCsAAxjhb2djKaL7LzQKbVRTHGmDfNAsQ4WjSzE4DNA8OoKgtu/RFffWhDi0tljDGHxwLEOFrU6wSITbuHyRacR5D+3c82trJIxhhz2CxAjKNZU9poT8bZvHvYni5njDnqWYAYR7GYsGBmJ5t3DzPqCxCq1mhtjDn6WIAYZ4tmdrJp4CCZfKk8bffBXAtLZIwxh8cCxDhbOLOTrftGq55PPZIrtLBExhhzeCxAjLNFvZ0US8rLOw+Up3kN1sYYczSxADHOFrpdXV/c4QsQ+RLrtw+yb9iqmowxR49IA4SIXC4iG0Rko4jcGjJfROTv3PnPisjZvnlbROQ5EXlaRPqiLOd48gLECzsGy9OyhSJX/d2jfPAbj7WqWMYYc8giCxAiEgduB64ATgU+IiKnBha7Alji/lsBfCMw/1JVPVNVl0VVzvHW05FiemeKF7YPlad5Ddabdg/z7ce2sGNwtFXFM8aYpkWZQZwLbFTVTaqaA+4BlgeWWQ7crY4ngB4RmRNhmY6IE3q7GMpUGqaHfY3U/+P+9bzjL3/WimIZY8whiTJAzAW2+t73u9OaXUaBh0RkrYisiKyUEThxdlfVe3+PJs/67YM88NwOG9jPGPOWFWWAkJBpwbNho2UuVNWzcaqhbhaRi0M/RGSFiPSJSN/AwMDhl3YcnTS7u+q9P5vw/PF9z/Hp765j9fM7ANg5lCFnvZ2MMW8hiQi33Q/M972fB2xvdhlV9f7fJSKrcKqsfhn8EFW9A7gDYNmyZW+Jy/GTZ0+peh+WQRzMOkFj275RcoUS533xYQDmTG3jo+cex2fes4ThbIFn+vfT25Vm/vQO2pLx6AtvjDGuKAPEGmCJiCwEtgHXAx8NLHM/cIuI3AOcBwyq6g4R6QRiqnrAfX0Z8BcRlnVcnTgrECAytQEiV3SyhWyhRKZQGZZjx2CG//3IJj7zniXc+chm/tdPXwYgJnDO8dO48aJF3LPmdTrTCT68bD7D2QLJeIxUIsa2faO8bW430zpSiMCUtiRT0gm27BkmnYzT25UmlbCezcaY5kQWIFS1ICK3AA8CceAuVV0vIje581cCq4ErgY3ACPAJd/VZwCoR8cr4PVX9cVRlHW9T25NV7wdDMojRnBMUsoViTdWSFzz2jeToSMX5y/9yOque2sZ/bBigK/06j726BwEyuSIPv7Srbjnm9rTzmXefwK33PQfA2cf18La5U/nJCztpT8XpSif47YsX8/WfvcLvvmcJV57u9A944Lkd/J8nXiOViHH92+dzxrwe/uahl4nH4LcvWcyPnt3BzZeewL1r+3l55wE+9/6TeHzTHqZ3pJg3rZ27H3+N+dM72Lp3hCWzuhCELXuGufL0OSyc2Ymq8vCLu+h2j9MbQxlmd7eRK5Q4fe5UHtk4wNT2JBedMJNNu4d5dddBkokY71g0o5xF7Rgc5aUdB4jHhERMaE/FmTetg650gmRc2DuSQxCKJWXgQJYTjumipEo6ESNXLNGejDOaL9KRSjCSK7BrKMus7jb2juTo7UqzZzjLjM40O4cyxGPCnKltiAilkiIC7neTA5k8uUKJmAg9Hcny9KBSSSmUtGGALpYUgartG9NKUWYQqOpqnCDgn7bS91qBm0PW2wQsjbJsR9LQaG0bhNcukSuUau60zhZKqCrZgnMCW37mXPYO5/iPDQMMjuY5cVYXyXgsNDPx27Z/lF0HsgCct3A6rw4M82z/IAVfw/gP+rby0hsH+OXLA+UA8cNnd7Du9X3ERIiJMDia5951/QDc/8x2MvkSJ87q4o/ufRaAy06bzSf+aQ0A//Oat/G1h1+pW54vXnM6/9LXX1436Npz5vGva53P+ukfXMKKu/vKz/n+8w+cxm9esACA3//+0zyxaW/N+qfO6eas43r47pOvV01ffuax/PvT2zn7uB7Wvb6fdy6ZySOv7Gb1776T2+57lmf6B2u25fff3n8SN196Aov+eDXvPeUYzjpuGn/1YPizPk6fO5Xntjnb2/KlqwC47o7HWbNlX9Vy7ck4px3bTToZ4/OXn8y133i8fHEwJZ3ggc++kxV3r+XkOVNY99o+Bkfz3HzpCXzlxxuY2pHkJ79/MT0dKW6991k27x7mrz+0lGv+4TGy+SJ/fNUp/O1PXyZXKJGIx/gvZ89lzea9PL11P8vPnMsfvO9EPnbnk4zkChzb0869n76AX//6o6y4eBEjuSL3rutn1e9cCMDN31vH06/vp+h+b5IJoVBU/vCyk/jqQxv43PtPKme6n7vsJP76oQ285+RZ3HjRQq5d+Rj3fvoCvvqTl/npCzs55/hpnDKnm3vX9SMI3e0JBkfzxET47HuXcN3bjwPgrkc30/faXuZP6+DB9W/wv39jGXN62vnA1x9lKJPn996zhNlT27ntvufobk+QzZdIJ2IUSso/feLtzOpu4/o7HmfO1Ha+/pGzuOYfHiNXKJLJl3jvKccQiwkPrd9JPCb8+QdO49KTjwEgky9y3Tcf50CmQKGkJGJCrlhianuSoUyeuAhXnzWX9duHeGH7EG3JGMf2tHPO8dP4wZqtiAgdKefiw7uYWTqvh9lT0zzw3BtkCyU+/a7FfOeJ18gXSyTjMf7Pjeehqnz8H5/kzPk9fOXapfz709v4qwc38KdXnUJMhL/44Quk4jGKqpRUuemSxSyc2ckf/uAZ4jHh7k+ey6Le6s4x4yHSADGZ3f3Jc3l55wG+uPrF8CqmQqWKKRsYGlzVySKy+RJtSeeKM51wvmyDo3mmdaRIxIW9TdyZnckXiceEU4/trro3w+NlN/7hybOFIot7u2hLxsnki+Vsx1muUm7/Z3j8ywZ587btr38fSP++kaqy7R/Nc8XbZvPA82+wf6RyHPeP5Dl34XQ+f/lJ7B3O81/vdu6lfGHHEG8M1T7Rb6c7bd3r+wF45JXdADy/fZDtg/WfAHjKnG76945UlfmnL+7i6a2VgPLJCxfy8Es7eW2PU3YvOICTFcRjUhMcAEbzRfpec6Zv3j1Mrljity5YwMFsgX9d28/re0d4YccQL+yo/N1+8fIAuWKJgQNZtu/P0NOR4p41W8vb2H3QuSBY/dwOdg5ly+v98Jkd5X1Y9dQ2PrD0WF7fO8Li3k6e7R9kz8EcL71xgD/4wTM15fzRsztCj83/+snLbB/MVK1z37ptbN07yrce28KUtgQ7h7L821PbuG/dNgB+vmGAPcM54iLsGc7xxlCG3ilphrMFnti0txwg/uKHLwCQSsTIFUq8sGOIgYPZ8sXCf//39UxJJziQLbD7YJaFMzvL8+58ZBO/ecECnt82xPPbhnh55wFe9B3Dn28YICYQiznftzVb9pYDxI7BTOjFQv++yt//b39afQH06sBw+fvkWTCjg427DgKwcddBprQlOOBeFP7pvz1fteymgYPu8DwHeXnnQb5y7VKe2LSX/n2jPLl5L3GRqs8H+M+Nu7nzkc3scL+7dz66mS9ec3ro3+nNsArpiFx8Yi+feuci0ol4aCO1J5uvziBibs1C1s0s0gkvQDj/D2UKpJMx2pLx0Kqrmu0XSrQlYqQT8aq2Dk8lQPhP+M7ntiVjZPLFMceSqg4u9Zdt5hkZ/iCQzRfJ5IvM7WknGZeq8mfyRWZ1t3HO8dN5x+IZY253MCSLA0Abl2vhzA6625MNl1l+5rEcN70jdF6zzwXxlvuvFy/iY+c5J8lsvvZY+o9P8O/p/6zgdyM45Ly37LtOck6M+0ZqLzbG6oIdtm/7m/hOZvJFzpjXw4zOFOB06pgztS10e15FW9ix8Hu3e4L31vJ/nwul2n3P5Eu8Y9EMOlKJwHd/fJ7j8u6TZ1W9b1RhmAm0QwLli8ZMvnYeOBeY/m1GVSFpASJi6WSscYAoVJ+AvXp5J3AUy5mDl64OjuZJJ+KkE7GmAkQmXySdjNOWjJEvOump3373xJANnHzbknHaEnHnCxry4/RvJlMnmwgry1j8+5QpFANlqQ5EbW7QbGui4X0w5ARY3laDk09bIk46GSObL1WdZP1NBG3JePnvFNR8gCi5nxcr/63DT8A53zqBAOH7O/gDCUCuWP13955X0uN+38ICRKZQbPgsk9Gw8vm2460a3IRT/RKjLeXsZzoRL2er9WQKxdpO8j49gXY//7ZqjlO+SKbgfq+SsZoLj/HQ01Fdnkah1gtYVdPcMmVD5jnrHJku8RYgIpZOxELvg/AEq5i629wA4QaOdLI6g8gVnGon5wc19pfEO5F6J53gRWFoBlH+8cTJFpwfUzxWfY1S7wfYOEA4n9Go/XX/SL48/0CmQEmhLRkjHdhfL3AAJOJjf43rXdkWVcv1/mHSvuBUbznn7xFehkyh1NQDo7yTrXfcnXXDTsD+DCtwUvEd+/2BE36wGtM7lj3uVfzgSO3xyeRr28fCyuzXzICUgyP5ctAHaE85J+qw7XntHpl8sea761d9QtaqYxG8QMsUSpULj0BgGq8TbzBA5Bt8xzJupuwplrRcDu8iKWwd/+GIqn+/BYiIpRPx8j0PYYKN1N3tTrOQEzhK5R9R2ncC8jKIZng/hHrLez+66quokntSjrkZRJGOZJyU70Q87GtrOOgLgI1+YGEnvKDRfLF8NegFL+9KL5uvLWMzprYnGanTNuL9+Ka0hTfHtfuuMuvtm/+kHrb9ZoZ7z/gCRLu7rbAyH8gU6EonqtbxeMenK52ouSg5EPgOeutOc09kYQE0ky/Wza660omqzCAZF1Lx8Iuh4MlrKFMo/02hkjWFHd9COUCUqrLcoJ6OVKDslW3t8wW/zlScXMHJiiuf2/gCpzMV/retN32s8gRl86Wq4JgtVNr96mXwYcE0ChYgIjbWiTwbDBBtlSqmTKFYDgz+E5CXQTQjWyiRSoy9vP9LmC0UaUv4Moh8iXQyXhWk/FVBVdVCTWQQY/F+XN7Vctr7IbsnCFUtZznNbS9Zd96we+Kst0xbMkZ7yjl5Ba/Cy8sk4vUziHz4FWDtciVS8RjxmJS3Va8K0etGHTxJeEEg2M06TKWKqfpY15S9zkk5+BleVZyfV50ZbAOA6qAadiUfVt5G35/g3686m8r7lqucuNvczGWsNojgyX6s6VAJvM0I7ttornLcR3P1M4gjwQJExII/miCnKqlOFVO+tpHaeX3oGcRYV9vBq/N0MkY6ESufGINBaahOgGh0tex9xljDT3knn3KASFb/kHPFEqo0HyAanDC9z/BOlEH+9o96V23pZKyc6QXVuwKsXa72YiDspA0wrTNZ3rafV03UKCB6vJ51XsYarJKqlD18n4OfkQ7JorzvQtg2/N8n7/VYHRwanRSDf7+qAOFrt/GXuy1R27YVFhDrHc9Gx7mZIO0va3UZKt+Z+lVM1gYxIczubm8436tK8nhfLC+l9l9leZyTd5MZhFddNMbywW6r/oZDf4Oe5/AyiMrNgY2UA4T7ww42Uns/jmaD5NQGV3pe1UqjDKJ8HOr8KNMNMrTsGCc2z3C2UN6Gt19hJ22onAyD290/kicek3IVVCP7h/NOdtQgGDW6ag9WyYW1w3hVj2HtG1VVTMk4bYnYmNlnoyrKzrT/+Et1j66qDMIXIJpsgzicANHe5MWL95nVF2jFMXsxjfUbGi8WICJ27TnzGs4PdnOttEEUQ7u5Oq/HzgjK2y8Um1q+qodQvlQ+KRdKynDWzSB8QcZ/8vK/zjTKILx7P8a4+vF6cnk/7GBdsffjGc8Mot4VX5tbtdboalpE6h7fTKF+NY3f4Gi+vA0RIZ2I1c0gpronpmAvo30juaoOCY3sH81VVfP4r7I9jYJb8ATobzvxf0a9bbclYsTc3gheORrVq2cD9+PUbM/32cVSqep76O+h5c80wjp7hFYx1cku600Plmcsznekugz+73ozvZiiGhXabpSL2HtOOabh/PpVTN59EJWugJ62ZIxkrNkqplLNyT18OacMxZLTq8d/Reh1rfXzN2r6Xx9scHd3JQNofMJsTzpjS+0PNFLvHfaqLErl6c1odKU36J686mYQVW0xzZ2g/JqtYto/mq/6G7Ul43V7XlVnmb6uraP5co+gMT9vJF/VbhDaBhHSe81fvur3lRO+/zPqbbu6Tc2rQmzczbVRFZR/e8Fgvq9hBlH9uWFBauphZBCHFCDyRQq+bsgZX6N1Jl8kFvI3CJYzqjYJyyAilozH+O1LFvneV/7Y3l2iofdBuF3xvMzB/6M/lAxiOFdw2izGyiDcMnh3ePuvLofcq1v/CcyftvtfN7pRyvuMsXr1eFUOXmbiXMVXGqm9/+sdg2C30jfTBuG1LzipfqXcwZ6r6Qa9mBpd+VbKkavpiFCviqnd7ZWWDfQyGhzJl6sGx/68fFU7QHgjdaluRlgTIBLxmosQr+oxrLE9rNNFxr3XJKxbcGgGV3UvSsy3bPVVd/0qprBeTLX7W6+HW+MA0fypNbhv/qzB3x7hVyxpVdfZqHo1WYA4Am674hT+5kPO0FJJX1fR7rZkTRtEuYrJ7R5ZqZeu/kE12wbhndzHWr5YUgrFyhfVufvavbp0r279P+p6GURYfbP/M/LF+lU1Hi84Vbq5Oidp7zhVyhi+T4XATWFvrg2icpXpL3cuUG1U72a9serOPftG8lUnlfZkvG4VU6VnVXX11b6RXFPZIlSqmMptECHVQI2CW/AE2J6qvQjxqnbCbsKrChC+71awV5+/LMHvjb9axb/P3n0OnqpGan8VU8LrwuzrwRfWoF7neI5XFVM2H6hi8lVL+tsjgoKjH0TBAsQRckx3Gqju297dnnB/ELVVTCO5IsWS1u/F1OQVSqXPee0X1vuReycJ/y3//l4p3tWt/6TgXd2mEtVXumMNtdCoN4qXXXn16N4JMlgVMFYVU/BqqlEGMdhEG0Rb0muLqX+/R7B6pbJc/R94sBzBapd6x9K7Wh8NnMD3jzrbaHf759erHgInIKWTcZJu19p9dTKIeo2hwQsOf+bifay3zbBt+79LibhUAkS+VBOUYhLeYO7/O/urYbzvmPcn8Qdaf3VRWyqkF1O+SPCwJeocx3pVT1B9IdhITCo9lbyPGRotoOrOa9B77oCvOteqmI5yF50wk4+ffxyfefcJ5WlOBlFdt9rp9kDxupF6gcD/A0gnms8gKsvX/qk7Us5nVeq0i76Tb6UNoqRuzynfCcy7eOtpT1Z1Wy2WtGF63ahO3gtUXrdWr/+8d4UZbMOo9znBfvf1qgigMrR6vT7t3phUUF1V0ujua79GN9gFy+H/m6aT8Zph4CvVjZVuv/4MIudmnN53plFgzBVKtPtuVAt7muFoo0bqwE1i/h5R09xjmQtUW1YvX1nf38g/GnLvxbSOVGgVU712Wa9x3SuH//c1rSOYQTjfK69aK5MvVS3TSDO9xcbS05Eq3+vgfQe9jGtaR4qSht9HAoGehxE9jdICxBEiIvz/V5/OH152UnnalLYE+aJWZRX+MZeg9krNW6bZDMJbPuxq27szujpAVKpv0oEr2rBUO+zKu1HwanTzVco37lRtHXWlKsB/13EzmlmuXhWT+NavV+XTSKPeT0H+gBdWZeVdETfqeuuvYmp0hessW9uFurrsDe4eTwQDRCXDHOtzveW9Z16Ib3thn9nTkXR6MTV5HL1sI+xvGmyDaE/FKWkl4I/miw0vKKr34c2fPns6kuXviFe2/YdwP4unmSz1cFiAaIF7VpzPR887jvMXOaOQ3rOm8uwC76T9rDvkcNiXMJ1orp7ZU+9GOS8pKXeb9PWKCQaEencLhwWIRj+cbIMr6qoTYOBqui0Rr7RhFCpZTjOaCRCNbmzyytLM4IhBzd4HAbU9e+pJlxvta4OPvz5/rL74bYnGAaJR2YPH3t/g3Shzqb9+Zfyp4Gf2dKQaBqsgL7PqSidqqtmmBXoxeVlZxte+FTwe9cYOG4+HOk3rSJWzTC9zGSy3izWXyUB0VUyRdnMVkcuBr+E8Ue5OVf1SYL6486/EeaLcb6nqumbWPZqdv2gG5y+awcFsgantSZ7t38/e4Rxb946yqLeT42d08KstzsNwjpnSVl5vanuy/DyIJnu5AtCRCs8gutuTbB/M0NvltI+896u/KM9rT1X3a29PxWrqZgGmddZ+iWd2paueRQBOUMsWSiz/+/+seyXYkaqc2NpS1RmEV6Vx/hcfrupp5enpSNa9wm/mpqV6V42JeGXU0e+7z10IUy+j++6Tr4cetzDBRuogZ2hqr3E5xpOb9lQ958DZRrxSdTRWgCgvF172bz22JbSLJYRUMSXi5ZNxMye2tkScDrd8ybiUn7R347f6apbtaU/y1Ov7QtsywjgPyspwxtwe2pPVY6F1t1cHCG8/lv/9o8Rjwo7BDEsCjwyu9xTAem0Th6KnPckjG3eDwjuXzAQoP5zLH2g7UvG644lBdI3UkQUIEYkDtwPvA/qBNSJyv6q+4FvsCmCJ++884BvAeU2ue9TrSif4+PnHA8dXTX/o9y9m73COYkmZ21O5E/uHn7mInUMZ3ja3G4Bvf/JcpnUkKZSUQlFZOn8qa7fsYyhTIJUQdh/Mkc0X+cDSuSTjMVZ+/BxG8wXOXTiD1/eM0J6K86vNe7j6zLksnT+VkZwzYmZnKs5Zx/UQF+G2K07mYLbAh5fNp1BS2pJxjpvewau7D7JwRicnz+lm4cxOervSDOcKjOaKfPS84/j5S7u46oxjWf3cDmIC7z5lFt954rVyA+SSWV3u07qUcxdM59WBg7Sn4vznK7u55MReZne3cdz0dhbO7CKdiHP522bTv2+03LVvZle66tj82+9cyMMv7WL7/lFyhRILZnaSjAszu9KcNHsKKy5eRFc6wZ6DWWZNbaN/3yjzprXz2u4RFszsZM7Udv78A6dxyYm9/N9ntnPN2XP5v8/s4Ozjelgwo4Mbzj+eTL7IMd1pDmYK5IolFszo5JzjpwFw+Wmz+f33noiiHMwUyici7yFCx8/o4MITZvDU6/vJFkrk3FFeh3NFSqoMjRb40Dnzy/tzwzuOpyMVZ0ZXimyhxImzpjC1Pcljr+7mgsUzaEvEeGDqG+XvUTIeY99Ijg+dM4/50ztYv32IK0+fw9Z9Iyw7fjr/snYrHak4gjCSK7JnOFt+OM+n33UCj7wywOypbew56DzMp6cjWX7w0gm9XZyzYFr5Ozmru425Pe188OxB5kxtY/v+Ua4+ay4AAweyfPS841jc28kbQ1mOm97O63tHmdmVoi0Zp6RKNl9i/vQO/vjKU5jRleK9p8xiOFvkumXzGXEvHi46YSbtqTgxEa46YzYd6QSqysKZnQjOg30KJWX+9A4udJ8H8vWPnMUruw6yefcwJVWuOn0O/ftGeKZ/kAUzOpjanqS3K81NlyxGVZnWkeTiJb1cc9bcchXTyXO6uezUWYzmiuw6kOX4GR28/7TZ7DmY40PL5nPvun72j+RYNLOLCxbP5OZLF1MoKUOjeXo6UrQn4+Waga9dfyYzOtOs2bKX/SM5PnjOPB5av5Ptg6PM7m5jWkeKE47pKgfyD799Pqce282mgWHSyRifvHAhU9uTKHDTJYu5b10/ibjwa2ccy12PbiZTKLFgRgevDhwsd24Zb9LMUMSHtWGRdwD/n6q+331/G4Cq/qVvmW8C/6Gq/+y+3wC8C1gw1rphli1bpn19tVcgxhhjwonIWlVdFjYvyjaIuYA/J+93pzWzTDPrAiAiK0SkT0T6BgYG3nShjTHGOKIMEGEVdMF0pd4yzazrTFS9Q1WXqeqy3t7eQyyiMcaYeqJspO4H5vvezwO2N7lMqol1jTHGRCjKDGINsEREFopICrgeuD+wzP3Ab4jjfGBQVXc0ua4xxpgIRZZBqGpBRG4BHsTpqnqXqq4XkZvc+SuB1ThdXDfidHP9RKN1oyqrMcaYWpH1YmoF68VkjDGHplW9mIwxxhzFLEAYY4wJNaGqmERkAHjtMFefCewex+IcDWyfJwfb58nhcPf5eFUNvUdgQgWIN0NE+urVw01Uts+Tg+3z5BDFPlsVkzHGmFAWIIwxxoSyAFFxR6sL0AK2z5OD7fPkMO77bG0QxhhjQlkGYYwxJpQFCGOMMaEmfYAQkctFZIOIbBSRW1tdnvEiIneJyC4Red43bbqI/EREXnH/n+abd5t7DDaIyPtbU+o3R0Tmi8jPReRFEVkvIr/nTp+w+y0ibSLyKxF5xt3nP3enT9h99ohIXESeEpEfuu8n9D6LyBYReU5EnhaRPndatPusqpP2H85AgK8Ci3CGGH8GOLXV5RqnfbsYOBt43jftK8Ct7utbgS+7r0919z0NLHSPSbzV+3AY+zwHONt9PQV42d23CbvfOM9O6XJfJ4EngfMn8j779v0PgO8BP3TfT+h9BrYAMwPTIt3nyZ5BnAtsVNVNqpoD7gGWt7hM40JVfwnsDUxeDnzbff1t4Grf9HtUNauqm3FG1z33SJRzPKnqDlVd574+ALyI8yTCCbvf6jjovk26/5QJvM8AIjIPuAq40zd5Qu9zHZHu82QPEE0/2nSCmKXO8zZw/z/GnT7hjoOILADOwrmintD77Va1PA3sAn6iqhN+n4G/Bf4IKPmmTfR9VuAhEVkrIivcaZHuc5RPlDsaNP1o0wluQh0HEekC7gU+q6pDImG75ywaMu2o229VLQJnikgPsEpE3tZg8aN+n0Xk14BdqrpWRN7VzCoh046qfXZdqKrbReQY4Cci8lKDZcdlnyd7BtHMY1Enkp0iMgfA/X+XO33CHAcRSeIEh++q6n3u5Am/3wCquh/4D+ByJvY+Xwh8QES24FQLv1tEvsPE3mdUdbv7/y5gFU6VUaT7PNkDxGR7tOn9wG+6r38T+Hff9OtFJC0iC4ElwK9aUL43RZxU4R+BF1X1q75ZE3a/RaTXzRwQkXbgvcBLTOB9VtXbVHWeqi7A+c3+TFU/zgTeZxHpFJEp3mvgMuB5ot7nVrfMt/ofziNPX8Zp5f+TVpdnHPfrn4EdQB7nauJGYAbwMPCK+/903/J/4h6DDcAVrS7/Ye7zRThp9LPA0+6/KyfyfgNnAE+5+/w88Gfu9Am7z4H9fxeVXkwTdp9xelo+4/5b752rot5nG2rDGGNMqMlexWSMMaYOCxDGGGNCWYAwxhgTygKEMcaYUBYgjDHGhLIAYcxbgIi8yxuV1Ji3CgsQxhhjQlmAMOYQiMjH3ecvPC0i33QHyjsoIn8jIutE5GER6XWXPVNEnhCRZ0VklTdWv4icICI/dZ/hsE5EFrub7xKRfxWRl0Tku9JgECljjgQLEMY0SUROAa7DGTTtTKAIfAzoBNap6tnAL4D/4a5yN/B5VT0DeM43/bvA7aq6FLgA5453cEaf/SzOWP6LcMYcMqZlJvtorsYcivcA5wBr3Iv7dpzB0UrA991lvgPcJyJTgR5V/YU7/dvAv7jj6cxV1VUAqpoBcLf3K1Xtd98/DSwAHo18r4ypwwKEMc0T4NuqelvVRJH/Hliu0fg1jaqNsr7XRez3aVrMqpiMad7DwLXuePze84CPx/kdXesu81HgUVUdBPaJyDvd6TcAv1DVIaBfRK52t5EWkY4juRPGNMuuUIxpkqq+ICJ/ivNUrxjOSLk3A8PAaSKyFhjEaacAZ/jllW4A2AR8wp1+A/BNEfkLdxsfOoK7YUzTbDRXY94kETmoql2tLocx482qmIwxxoSyDMIYY0woyyCMMcaEsgBhjDEmlAUIY4wxoSxAGGOMCWUBwhhjTKj/Bx3dd2VjMlFwAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa09d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbc1b1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 40000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = enumerate(testloader)\n",
    "batch_idx, (Tensorx_test, Tensory_test) = next(examples)\n",
    "Tensorx_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e220023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0046)\n",
      "Error: 0.0046 \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    \n",
    "    for i, (Tensorx_test, Tensory_test) in enumerate (testloader):\n",
    "        total = 1\n",
    "        output = model(Tensorx_test)               \n",
    "        \n",
    "\n",
    "        correct += torch.mean(Tensory_test - output).clone().detach()\n",
    "        print(correct)\n",
    "        if (i+1) % 1 == 0:\n",
    "            print('Error: {:.4f} '.format(correct / total))\n",
    "print(len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff97590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.004612521268427372\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    \n",
    "    for i, (Tensorx_test, Tensory_test) in enumerate (testloader):\n",
    "        total += i\n",
    "        output = model(Tensorx_test)               \n",
    "        \n",
    "\n",
    "        correct += torch.mean(Tensory_test - output).clone().detach()\n",
    "        \n",
    "\n",
    "    print('Error:{}'.format(correct / total))\n",
    "print(len(testloader.dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "120a7cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "trainloader = torch.rand(1,200,200)\n",
    "transform = T.ToPILImage()\n",
    "img = transform(trainloader)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d8b206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
